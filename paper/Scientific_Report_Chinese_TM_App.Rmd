---
title: 'Whats driving chinese news'
author: 'David Schulze, Eyayaw Teka Beze, Nils Paffen'
subtitle: "How news correlates with the economy "
type: "Scientifc Report"
discipline: "Phd Programm RWI, VWL M.Sc.,"
date: "today"
studid: "3071594, ID David, ID Eyayaw"
supervisors: "Martin Christopher Arnold and Alexander Gerber"
ssemester: "3"
estdegree_emester: "Winter Term 2020"
deadline: "Apr. 7th 2020"
header-includes:
   # fix for latex unicode error with Chinese characters
   - \usepackage{CJKutf8}
   - \usepackage[utf8]{inputenc}
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 12pt
geometry: lmargin = 3 cm,rmargin = 2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
---

% This command is used by pandoc when create lists and is defined in pandoc's default latex template
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



# Introduction (David)

- short intro

- motivation: historic impact of news

- intro to structure of People's Daily

- motivation: People's Daily is the governing Chinese Communist Party's Major Outlet


## Current Situation in China (David)

- End of 2019: Trade war with the US

- 2020: Covid-19



# Data Sources 

## Text Data (Nils)

- short and general info

About one year and three month of data. Scraped from the free archive of [Peoples Daily](http://paper.people.com.cn/rmrb). A more detailed explanation of the scraping process is added in the method section \ref{sec:Methods} The app includes an automated updating routine that expands the database scraping directly from paper.people.com.cn.

## Economic Data (Eyayaw)



# Construction of the Application

## Concept (David)
- introduce file structure
- intruduce app concept in detail, maybe use ((Scrivner, Olga & Davis, Jefferson. (2017). Interactive Text Mining Suite: Data Visualization for Literary Studies)) available at http://ceur-ws.org/Vol-1786/scrivner.pdf for cool info on shiny text mining.


## Scraping process for original database (Nils)
Webscraping with Rselenium and Docker
The database of crossasia.org provides access to all editions of Remin Ribao since 1946, the oldest edition being dated May 15, 1946.
Since a larger database offers more possibilities to compare current and historical events, we had the idea to develope a webscraper, which adds the articles of this archive to our database. Since this archive is only accessable with an account and a memebership a login for the scraping process is necessary. The best solution to send the login credentials, browse through the archive, and scrape the content, we came up with is an firefox image in docker accessed by the RSelenium package.   

Docker is a batch of so called platform as a service (PaaS), which is a type of cloud computing service to manage and run application without any kind of infrastructure. For our purpose we used docker to build a browser in a virtual machine and then connect these with R using the RSelenium package. For an detailed installation guideline we recommend to have a look at https://docs.ropensci.org/RSelenium/articles/docker.html. To interact with the virtual browser image in R, RSelenium uses a class called RSelenium::remoteDriver, which features several functions such as remDr$navigate("url") to browse to a specific website or more specific ones like remDr$screenshot() to display or save and screenshot of the actual webpage, or remDr$sendKeysToElement() which allows the user to send specific keys like "enter" to the browser. 


Web scraping of the archive of Remin Ribao via crossasia.org : 
 The article overview of each issue is uniformly displayed by the date that the issue page is reached. In this case, 19460515 for the date and 1 for page 1. The link structure of the archive differs significantly from that of the free archive. Besides a personal session token in this case "s894ib9c043f", the link structure differs by the access points to the archive. In addition to crossasia.org, the Berlin State Library should be mentioned here in particular. This can be found in the suffix ".erf.sbb.spk-berlin.de".   The article overview of each issue is uniformly accessible via the date and page number. 
LINK http://data.people.com.cn.s894ib9c043f.erf.sbb.spk-berlin.de/rmrb/19460515/1/ 
In this case, 19460515 for the date and 1 for page 1. 32-character ID is used for article archiving. LINK : http://data.people.com.cn.s894ib9c043f.erf.sbb.spk-berlin.de/rmrb/19460515/1/cae1d1ef94a74c8e99ad43189652fb40 shows the oldest article of the archive. Since the ID does not seem to follow a fixed pattern, it must be obtained from the article overview for the scraping process. A list of all article IDs can be retrieved on each overview page by using the HTML node "li h3 a" and the attribute "href". Now links can be generated from the date format and the IDs to call up the individual articles. Almost all important information such as author, title, subtitle and article content can be retrieved via the html node "#detail_pop_content" and the underlying nodes ".title", ".subtitle", ".author", and "p".  

Page protection 
Since crossasia.org as the provider of the archive offers access for a fee, they are oriented towards protecting their archive data. Although the use for preview purposes is allowed, the provider cannot always be distinguished from a "data pirate", i.e. offering the articles on the Internet after downloading them. In addition, the provider wants to keep his server load as low as possible in order to grant as many users as possible quick access to the archive at the same time. If several users now send automated requests to the archive, this can affect the speed of the servers. By randomly generating sleep times in the code, the scraping behavior can be adapted to the usage behavior of a human being. crossasia.org asks the user to insert a sinmplen captcha PICTURE after a certain number of article calls. This problem could be overcome by first searching for a location in the HTML code that is present on both the article page and the captcha page.  Then a screenshot of the page was taken by the remoteDriver and cut to the captcha using the image_crop() function from the magick package. The page https://2captcha.com LINK offers the possibility to solve captchas automatically for a small fee of less than 1$ per 1000 capchas. For this purpose, the user is provided with an API key, which can be used to transmit the image information as a POST request with multipart-encoding. Afterwards the user is assigned a ticket-ID, with which he can retrieve the solved captcha code as a string after about 5-10 seconds of waiting time.  Since the operator may sometimes experience increased solving times, we check the content of the string. A waiting time of 5 seconds is added if the captcha has not been solved yet and an attempt to grab the code follows the additional waiting time. The captcha-code is now sent to the to the corresponding form of crossasia.com and will be forwarded automatically. After entering the captcha code, we check again if the forwarding was successful or if the entered captcha code was invalid and therefore another captcha must be solved. Even solving these captures codes and extensive waiting time of about 10 minutes after each page, we expierenced that the archive operator time outs user addiotionally who try so look at to many articles in a given time. The timeout length and the length until a timeout is called differs from 30 minutes to a few hours. We tried to check for this site behaviour but were not completly succesfull by providing a function to check for the timeout and wait until it disappears. 

Output
In the end the intense timeout periods lead us to the decision that the webscraping of the archive is possible but extreme time consuming. The code will be able to scrape the content as described but for the purpose of the app we decided to not integrate the archive content so far. 

## Translation from Chinese (David)
To make the app basically usable for non-Chinese speakers, a translation feature was introduced. Because Chinese has no spaces, we needed to use a NLP algorithm ((quote: jiebaR)), so that in the article texts, spaces could be placed between words to separate them. Unique words were added to a Chinese dictionary. Each word was in turn passed on to the translation API from Yandex. This yields in most cases a reliable one-word translation. Alternative translations were beyond the scope of this app, and for in depth analysis, a knowledge of Chinese or at least use of more translation tools is necessary. This dictionary is exported in .rds and .csv files for further use. The articles are then translated word for word. This will speed up using the app for text mining with English words, even if it does not give a useful translation of the complete texts.

### Some additional remarks:

To reduce use of the Yandex translation API (which has limited free bandwith) in each update, only new words are translated and added. This allowed an approximation of the range of unique words used. For illustration: Translating all the unique words (words includes here names, idioms, technological terms, etc.) yielded at least 50150 unique words for the first pages of all of 2019. The second pages of 2019 added 16393 new words. For the first three months, we found still 3228 new words for page 1 and 2350 new words for page 2. Even when allowing for misclassification by the NLP algorithm (sometimes words might be recorded as separately and again as a compound), this suggests that the news needs plenty of new words to adapt to current developments, e.g. "Covid-19".

Another aspect of the dictionary: While all Chinese words are unique, the English translation yielded around 25000 duplicated entries. This means that users searching for example for "satellite" will find hits for 卫星, 颗卫星 and 人造卫星, which all carry that meaning, improving the accuracy of text mining.


## Economic data retrieval (Eyayaw)

- Where, what and how did we get it.

- This can be very short.


## Exploratory statistics, visualisation, and text mining (Eyayaw)
Start with simplest descriptives and plots, e.g.:

- descriptive plots for the data: article frequencies etc.
The data reveal that there are systematic differences between the first and second pages. ((insert article frequency plots from descriptives))
For both 2019 and 2020, the first page has several outliers with over 10 articles per page, while the second page has no more than 9 articles per page. This is mostly due to the coverage over important events which are covered in many small articles on the front page, e.g Xi Jinping's meetings with world leaders at international summits.

- most common non-stop words with frequencies

- compare pre- and post-corona common words

- pre-post word clouds with translated words

- two-word correlation

- time-series frequency plots (very important)

- n-grams (possiblly useful packages: JSTORr, ngram, NSP, WordStat)


Optional:

- bigram graph

- two-word correlation with phi-coefficient

- lda/topic model (very optional)

Do not:

- sentiment analysis


## Shiny user interface (David)



# Results

Here we should show some of the results of our application. As discussed before COVID-19 will be most likely the centre of our examples but maybe we can show here the mentioned theory of the shadow page as well



# Discussion

How could we improve the app? Extend the archive? Add more economic data? Implement some other statistical information?

Encrypted server version



# Conclusion

Summary of our application and our results. Do we have plans to expand and update the app further in the near future?



Just an idea, because they want it scientific, let's include a bunch of references to the data mining book, etc., just if you find some that would make sense.

\newpage











