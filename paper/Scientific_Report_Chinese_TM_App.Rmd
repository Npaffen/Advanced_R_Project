---
title: 'Whats driving chinese news'
author: 'David Schulze, Eyayaw Teka Beze, Nils Paffen'
subtitle: "How news correlates with the economy "
type: "Scientifc Report"
discipline: "Phd Programm RWI, VWL M.Sc.,"
date: "today"
studid: "3071594, ID David, ID Eyayaw"
supervisors: "Martin Christopher Arnold and Alexander Gerber"
ssemester: "3"
estdegree_emester: "Winter Term 2020"
deadline: "Apr. 7th 2020"
header-includes:
   # fix for latex unicode error with Chinese characters
   - \usepackage{CJKutf8}
   - \usepackage[utf8]{inputenc}
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 12pt
geometry: lmargin = 3 cm,rmargin = 2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
---

% This command is used by pandoc when create lists and is defined in pandoc's default latex template
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



# Introduction (David)


## Motivation

Ever since it's establishment in 1948 as the Chinese Communist Party's (CPC) officially designated publication organ, and especially since it announced the foundation of the People's Republic by Mao Zedong on October 1st, 1949, the People's Daily newspaper (also known as Renmin Ribao or RMRB) has been an object of the highest interest for anyone interested in modern China. It is safe to say that it's front page is that of the most widely published newspapers in Chinese and it's the most widely read. This amount of attention and it's clear designation as voice of the CPC have made it an invaluable source for information on China's ruling elite's communication with the masses. In times of crisis, even tiny changes in placing, formatting or wording are chosen and interpreted with extreme scrutiny @tan1990. The reason for this is on the one hand the need for some form of discussion involving the government and the educated citizenry @kuhn2002f, but also the sensitivity of certain topics, also called censorship.

In the past, close reading and an intimate knowledge of the Chinese language were the only tools available to researchers interested in this publication. Even the most well-read scholars of modern China will have to admit that reading the paper daily and in it's entirety, even just the front page, will be a thankless undertaking. Most articles are official statements and collections of facts about the activites of leaders, or plain positive messages about some aspect about the nation, also called, by the CPC themselves, propaganda. The nuances that to detect require years of study are difficult to check against factual evidence, short of spending hours of reading yourself. Only rarely are messages communicated as clearly, as back on the founding day of the People's Republic.

This presents in our view a very urgent opportunity for automated data mining. Unlike historic literary corpora like the works of Shakespeare that can be analysed by generations of scholars, news is by its nature fleeting and often needs to be analysed in a hurry and theories tested as events unfold.

To test our idea for an automated evaluation of People's Daily articles themselves and in context of economic data, we proposed the development of an app, that would automate the task of updating the news corpora and economic data, as well as some descriptive and basic analytical steps used in quantitative text analysis. To make these results available to non-Chinese speakers, we include a simple translation routine, that gives the most common word for word translation, even if not the meaning of entire articles or sentences.

We stress that this will in no way substitute any qualitative reading, knowledge of Chinese politics, expertise in Chinese language or even text mining of the news in general. But to be able to quickly develop and test quantitative hypotheses about Chinese news and its relation to economic data, might be a useful tool on the way to further research and in-depth text mining.

## Newspaper Structure

For the app we focus on the first two pages, because the first is arguably one of the most important daily publications in China, and the second might offer interesting contrasts, because it is aimed at a different audience, while significance diminishes rapidly with increasing page count: The latter pages of this leading CPC publications are actually full-page advertisements for private company's products, among others.

The first page's layout is different every day, according to the needs of the editors: The documentory function implies that a lot of information is squeezed into a tight space. Articles may be reduced, truncated or pushed to the side of the page to make room for symbolic pictures ^[See for example the pictures displaying national remembrance of the "martyrs sacrificed in the struggle against Covid-19" with bowing national leaders and the lowered flag in black and white [front page from 2020-04-05](http://paper.people.com.cn/rmrb/html/2020-04/05/nw.D110000renmrb_20200405_1-01.htm)] and voluminous leading articles. On the other end of the spectrum are crowded front pages with up to 15 tiny articles commemorating each meeting of a tightly packed international summit schedule, with 15 headlines reading: "Xi Jinping meets (insert name of foreign national head of state)" ^[[front page from 2020-04-26](http://paper.people.com.cn/rmrb/html/2019-04/26/nbs.D110000renmrb_01.htm)].

How can we ever expect to extract useful information from such a data source? Well, for nuances in reporting to have any impact, they have to be special or deviating in some way from an established norm. By gathering data over hundreds of days, we hope that these patterns and deviations will become visible. Also, comparing subsets of data such as front and second page, reporting before and after a certain date, commonalities will be filtered out and differences highlighted. Since our data cover the year 2019 and the first months of the Covid-19 outbreak that was first documented officially by authorities in Wuhan, Hubei Province China on 2020-01-05 ^["Wuhan Health Commission report on the situation concerning a viral lung disease with unknown origins", [wjw.wuhan.gov.cn](http://wjw.wuhan.gov.cn/front/web/showDetail/2020010509020)], this constitutes an opportunity to evaluate how this major shock is covered and news compare before and after the outbreak was acknowledged.

While text mining and natural language processing algorithms have matured extremely in the contexts of machine learning and applications for example in online search, the technology barrier has limited it's adoption in the social sciences. To make basic results accessible and reproducible, is finally a large motivation for the creation of this app.



# Data Sources 

## Text Data (Nils)

- short and general info

About one year and three month of data. Scraped from the free archive of [Peoples Daily](http://paper.people.com.cn/rmrb). A more detailed explanation of the scraping process is added in the method section \ref{sec:Methods} The app includes an automated updating routine that expands the database scraping directly from paper.people.com.cn.

## Economic Data (Eyayaw)



# Construction of the Application

## Concept (David)
- introduce file structure
- intruduce app concept in detail, maybe use ((Scrivner, Olga & Davis, Jefferson. (2017). Interactive Text Mining Suite: Data Visualization for Literary Studies)) available at http://ceur-ws.org/Vol-1786/scrivner.pdf for cool info on shiny text mining.


## Scraping process for original database (Nils)
Webscraping with Rselenium and Docker
The database of crossasia.org provides access to all editions of Remin Ribao since 1946, the oldest edition being dated May 15, 1946.
Since a larger database offers more possibilities to compare current and historical events, we had the idea to develope a webscraper, which adds the articles of this archive to our database. Since this archive is only accessable with an account and a memebership a login for the scraping process is necessary. The best solution to send the login credentials, browse through the archive, and scrape the content, we came up with is an firefox image in docker accessed by the RSelenium package.   

Docker is a batch of so called platform as a service (PaaS), which is a type of cloud computing service to manage and run application without any kind of infrastructure. For our purpose we used docker to build a browser in a virtual machine and then connect these with R using the RSelenium package. For an detailed installation guideline we recommend to have a look at https://docs.ropensci.org/RSelenium/articles/docker.html. To interact with the virtual browser image in R, RSelenium uses a class called RSelenium::remoteDriver, which features several functions such as remDr$navigate("url") to browse to a specific website or more specific ones like remDr$screenshot() to display or save and screenshot of the actual webpage, or remDr$sendKeysToElement() which allows the user to send specific keys like "enter" to the browser. 


Web scraping of the archive of Remin Ribao via crossasia.org : 
Before we can start to scrape the articles we need to start the docker image as explained in the guideline. Then the first thing scraper function ca_scaper() function calls is ca_login(). The latter connects the remoteDriver with docker. When this is done it navigates to the login page of crossasia.org and hand over our account credentials.

 The article overview of each issue is uniformly displayed by the date that the issue page is reached. In this case, 19460515 for the date and 1 for page 1. The link structure of the archive differs significantly from that of the free archive. Besides a personal session token in this case "s894ib9c043f", the link structure differs by the access points to the archive. In addition to crossasia.org, the Berlin State Library should be mentioned here in particular. This can be found in the suffix ".erf.sbb.spk-berlin.de".   The article overview of each issue is uniformly accessible via the date and page number. 
LINK http://data.people.com.cn.s894ib9c043f.erf.sbb.spk-berlin.de/rmrb/19460515/1/ 
In this case, 19460515 for the date and 1 for page 1. 32-character ID is used for article archiving. LINK : http://data.people.com.cn.s894ib9c043f.erf.sbb.spk-berlin.de/rmrb/19460515/1/cae1d1ef94a74c8e99ad43189652fb40 shows the oldest article of the archive. Since the ID does not seem to follow a fixed pattern, it must be obtained from the article overview for the scraping process. A list of all article IDs can be retrieved on each overview page by using the HTML node "li h3 a" and the attribute "href". Now links can be generated from the date format and the IDs to call up the individual articles. To fullfill this task the scraper calls the function ca_date_urls. The so created url list is then passed to ca_content().

Afterwards the scraper maps over this list while calling the ca_content()-function. 
Almost all important information such as author, title, subtitle and article content can be retrieved via the html node "#detail_pop_content" and the underlying nodes ".title", ".subtitle", ".author", and "p". After scraping the information all content is saved in a tibble. 

Page protection 
Since crossasia.org as the provider of the archive offers access for a fee, they are oriented towards protecting their archive data. Although the use for preview purposes is allowed, the provider cannot always be distinguished from a "data pirate", i.e. offering the articles on the Internet after downloading them. In addition, the provider wants to keep his server load as low as possible in order to grant as many users as possible quick access to the archive at the same time. If several users now send automated requests to the archive, this can affect the speed of the servers. By randomly generating sleep times in the code, the scraping behavior can be adapted to the usage behavior of a human being. crossasia.org asks the user to solve a simple captcha after a certain number of article calls. To solve these proble, our webscraper first checks for a location in the HTML code that is present on both the article page and the captcha page. If the site structure equals the one of a saved version of the captcha site the function ca_captcha() is called. 

First, a screenshot of the page is saved by the remoteDriver and cropped in a way that only the captcha image is left. The page https://2captcha.com LINK offers the possibility to solve captchas automatically for a small fee of less than 1$ per 1000 capchas. For this purpose, the user is provided with an API key, which can be used to transmit the image information as a POST request with multipart-encoding. Afterwards the user is assigned a ticket-ID, with which he can retrieve the solved captcha code as a string after about 5-10 seconds of waiting time.  Since the operator may sometimes experience increased solving times, we check the content of the string. A waiting time of 5 seconds is added if the captcha has not been solved yet and an attempt to grab the code follows the additional waiting time. The captcha-code is now sent to the to the corresponding form of crossasia.com and will be forwarded automatically. After entering the captcha code, we check again if the forwarding was successful or if the entered captcha code was invalid and therefore another captcha must be solved. Even solving these captures codes and extensive waiting time of about 10 minutes after each page, we expierenced that the archive operator time outs user addiotionally who try so look at to many articles in a given time. The timeout length and the length until a timeout is called differs from 30 minutes to a few hours. We tried to check for this site behaviour but were not completly succesfull by providing a function to check for the timeout and wait until it disappears. 

Output
In the end the intense timeout periods lead us to the decision that the webscraping of the archive is possible but extreme time consuming. The code will be able to scrape the content as described but for the purpose of the app we decided to not integrate the archive content so far. 

## Translation from Chinese (David)
To make the app basically usable for non-Chinese speakers, a translation feature was introduced. Because Chinese has no spaces, we needed to use a NLP algorithm ((quote: jiebaR)), so that in the article texts, spaces could be placed between words to separate them. Unique words were added to a Chinese dictionary. Each word was in turn passed on to the translation API from Yandex. This yields in most cases a reliable one-word translation. Alternative translations were beyond the scope of this app, and for in depth analysis, a knowledge of Chinese or at least use of more translation tools is necessary. This dictionary is exported in .rds and .csv files for further use. The articles are then translated word for word. This will speed up using the app for text mining with English words, even if it does not give a useful translation of the complete texts.

### Some additional remarks:

To reduce use of the Yandex translation API (which has limited free bandwith) in each update, only new words are translated and added. This allowed an approximation of the range of unique words used. For illustration: Translating all the unique words (words includes here names, idioms, technological terms, etc.) yielded at least 50150 unique words for the first pages of all of 2019. The second pages of 2019 added 16393 new words. For the first three months, we found still 3228 new words for page 1 and 2350 new words for page 2. Even when allowing for misclassification by the NLP algorithm (sometimes words might be recorded as separately and again as a compound), this suggests that the news needs plenty of new words to adapt to current developments, e.g. "Covid-19".

Another aspect of the dictionary: While all Chinese words are unique, the English translation yielded around 25000 duplicated entries. This means that users searching for example for "satellite" will find hits for "Wei Xing", "Ke Weixing" and "Renzao Wei Xing", which all carry that meaning, improving the accuracy of text mining.


## Economic data retrieval (Eyayaw)

- Where, what and how did we get it.

- This can be very short.


## Exploratory statistics, visualisation, and text mining (Eyayaw)
Start with simplest descriptives and plots, e.g.:

- descriptive plots for the data: article frequencies etc.
The data reveal that there are systematic differences between the first and second pages. ((insert article frequency plots from descriptives))
For both 2019 and 2020, the first page has several outliers with over 10 articles per page, while the second page has no more than 9 articles per page. This is mostly due to the coverage over important events which are covered in many small articles on the front page, e.g Xi Jinping's meetings with world leaders at international summits.

- most common non-stop words with frequencies

- compare pre- and post-corona common words

- pre-post word clouds with translated words

- two-word correlation

- time-series frequency plots (very important)

- n-grams (possiblly useful packages: JSTORr, ngram, NSP, WordStat)


Optional:

- bigram graph

- two-word correlation with phi-coefficient

- lda/topic model (very optional)

Do not:

- sentiment analysis
## Word frequency time series and economic data
In order to gain an impression of the impact the frequency of a single word can have on the economy, we created a function to generate time series and visualize the results. As mentioned in the section of the ecomomic data we implemented a function 
## Shiny user interface (David)



# Results

Here we should show some of the results of our application. As discussed before COVID-19 will be most likely the centre of our examples but maybe we can show here the mentioned theory of the shadow page as well



# Discussion (David)

How could we improve the app? Extend the archive? Add more economic data? Implement some other statistical information?

Encrypted server version



# Conclusion (David)

Summary of our application and our results. Do we have plans to expand and update the app further in the near future?



Just an idea, because they want it scientific, let's include a bunch of references to the data mining book, etc., just if you find some that would make sense.

\newpage











